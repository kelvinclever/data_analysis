{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def process_league_season(league, season, base_url):\n",
    "    url = f'{base_url}/{league}/{season}'\n",
    "    try:\n",
    "        # Implement exponential backoff for robust scraping\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                with requests.Session() as session:\n",
    "                    # Add user agent to mimic browser request\n",
    "                    headers = {\n",
    "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "                    }\n",
    "                    res = session.get(url, headers=headers, timeout=15)\n",
    "                    \n",
    "                    # Check for successful response\n",
    "                    res.raise_for_status()\n",
    "                    \n",
    "                    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "                    \n",
    "                    # More robust script finding\n",
    "                    scripts = soup.find_all('script', string=lambda text: text and 'teamsData' in text)\n",
    "                    if not scripts:\n",
    "                        print(f\"No data found for {league} {season}\")\n",
    "                        return None\n",
    "                    \n",
    "                    # Extract JSON data\n",
    "                    string_with_json_obj = scripts[0].text.strip()\n",
    "                    \n",
    "                    # More robust JSON extraction\n",
    "                    ind_start = string_with_json_obj.index(\"('\") + 2\n",
    "                    ind_end = string_with_json_obj.index(\"')\")\n",
    "                    json_data = string_with_json_obj[ind_start:ind_end].encode('utf8').decode('unicode_escape')\n",
    "                    data = json.loads(json_data)\n",
    "                    \n",
    "                    break  # Success, exit retry loop\n",
    "            \n",
    "            except (requests.RequestException, ValueError) as e:\n",
    "                # Exponential backoff\n",
    "                if attempt < 2:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    print(f\"Failed to process {league} {season}: {e}\")\n",
    "                    return None\n",
    "        \n",
    "        # Process teams data\n",
    "        teams_data = {}\n",
    "        for team_id, team_info in data.items():\n",
    "            team_name = team_info['title']\n",
    "            team_history = team_info['history']\n",
    "            \n",
    "            # Efficient data processing\n",
    "            processed_rows = [list(row.values()) for row in team_history]\n",
    "            df = pd.DataFrame(processed_rows, columns=list(team_history[0].keys()))\n",
    "            \n",
    "            # Vectorized calculations with robust error handling\n",
    "            df['ppda_coef'] = df['ppda'].apply(lambda x: x['att']/x['def'] if x['def'] != 0 else 0)\n",
    "            df['oppda_coef'] = df['ppda_allowed'].apply(lambda x: x['att']/x['def'] if x['def'] != 0 else 0)\n",
    "            \n",
    "            # Aggregate statistics\n",
    "            sum_cols = ['xG', 'xGA', 'npxG', 'npxGA', 'deep', 'deep_allowed', 'scored', 'missed', 'xpts', 'wins', 'draws', 'loses', 'pts', 'npxGD']\n",
    "            mean_cols = ['ppda_coef', 'oppda_coef']\n",
    "            \n",
    "            sum_data = df[sum_cols].sum()\n",
    "            mean_data = df[mean_cols].mean()\n",
    "            \n",
    "            final_data = pd.concat([sum_data, mean_data])\n",
    "            final_data['team'] = team_name\n",
    "            final_data['matches'] = len(df)\n",
    "            \n",
    "            teams_data[team_name] = final_data\n",
    "        \n",
    "        # Convert to DataFrame more efficiently\n",
    "        full_stat = pd.DataFrame.from_dict(teams_data, orient='index').reset_index()\n",
    "        \n",
    "        # Additional transformations\n",
    "        full_stat = full_stat.rename(columns={'index': 'team'})\n",
    "        full_stat['season'] = season\n",
    "        full_stat['league'] = league\n",
    "        full_stat['position'] = range(1, len(full_stat) + 1)\n",
    "        \n",
    "        # Vectorized calculations\n",
    "        full_stat['xG_diff'] = full_stat['xG'] - full_stat['scored']\n",
    "        full_stat['xGA_diff'] = full_stat['xGA'] - full_stat['missed']\n",
    "        full_stat['xpts_diff'] = full_stat['xpts'] - full_stat['pts']\n",
    "        \n",
    "        # Select and order columns\n",
    "        col_order = [\n",
    "            'league', 'season', 'position', 'team', 'matches', \n",
    "            'wins', 'draws', 'loses', 'scored', 'missed', 'pts', \n",
    "            'xG', 'xG_diff', 'npxG', 'xGA', 'xGA_diff', 'npxGA', \n",
    "            'npxGD', 'ppda_coef', 'oppda_coef', \n",
    "            'deep', 'deep_allowed', 'xpts', 'xpts_diff'\n",
    "        ]\n",
    "        \n",
    "        return full_stat[col_order]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Comprehensive error in {league} {season}: {e}\")\n",
    "        return None\n",
    "\n",
    "def comprehensive_data_collection(leagues, seasons, base_url):\n",
    "    # Use ThreadPoolExecutor for concurrent processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Create futures for each league-season combination\n",
    "        futures = {\n",
    "            executor.submit(process_league_season, league, season, base_url): \n",
    "            (league, season) \n",
    "            for league in leagues \n",
    "            for season in seasons\n",
    "        }\n",
    "        \n",
    "        # Collect results\n",
    "        full_data = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    full_data.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Processing error: {e}\")\n",
    "        \n",
    "        # Combine all collected data\n",
    "        if full_data:\n",
    "            return pd.concat(full_data, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Configuration\n",
    "base_url = 'https://understat.com/league'\n",
    "leagues = ['La_liga', 'EPL', 'Bundesliga', 'Serie_A', 'Ligue_1', 'RFPL']\n",
    "\n",
    "# Dynamic season generation\n",
    "current_year = datetime.now().year\n",
    "seasons = [str(year) for year in range(2014, current_year + 1)]\n",
    "\n",
    "# Collect and process data\n",
    "comprehensive_football_data = comprehensive_data_collection(leagues, seasons, base_url)\n",
    "\n",
    "# Optional: Save to CSV for further analysis\n",
    "comprehensive_football_data.to_csv(r'C:\\Users\\kevo\\Desktop\\Data_Analysis_Projects\\epl_webscraping\\comprehensive_football_stats.csv', index=False)\n",
    "\n",
    "# Display summary\n",
    "print(comprehensive_football_data.groupby(['league', 'season']).size())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
